{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA5hn9X0CuMVAM6ya6MAm6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InvoLab101/10conv-poc/blob/main/10conversation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "ev5bXjxcWtPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Install dependencies"
      ],
      "metadata": {
        "id": "KyQ-sKEF10nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install diskcache"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_kzyPyB101q",
        "outputId": "9b812b0c-e4f8-4e0e-828e-cf332d11db38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m747.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n",
            "Collecting diskcache\n",
            "  Downloading diskcache-5.6.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m798.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache\n",
            "Successfully installed diskcache-5.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### load data"
      ],
      "metadata": {
        "id": "BptSitXmx_Xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import openai\n",
        "import logging\n",
        "import diskcache\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "conversation_folder = '/content/drive/MyDrive/10conversations'  # Change this to your desired directory\n",
        "conversation_files = os.listdir(conversation_folder)\n",
        "QUESTIONS = [\n",
        "    'Did the HDA welcome the customer to the bank, introduced himself/herself by name, and attained customer\\'s name immediately? (Yes or No)',\n",
        "    'Did the HDA acknowledge the customer’s request, paraphrased back to demonstrate active listening and understanding of the customer’s request or needs? (Yes or No)',\n",
        "    'Did the HDA demonstrate empathy of customer\\'s concern (Yes or No)',\n",
        "    'Did the HDA make strong/confident customer advocacy statements? (Yes or No)',\n",
        "    'Did the HDA promote passcode for future interactions? (Yes or No or NA)',\n",
        "    'Did the HDA gain permissions to ask questions and make value added recommendations? (Yes or No)',\n",
        "    'Did the HDA solve the customer’s request efficiently and position alternatives with customer benefits and educate customer as needed? (Yes or No)',\n",
        "    'Was a call transfer absolutely necessary to resolve the customer’s request? (Yes or No or NA)',\n",
        "    'Did the HDA add value by promoting or offering or acknowledging digital banking use and services? (Yes or No)',\n",
        "    'Did the HDA use benefit statements to explain why the product or service is of value to the customer’s needs and ask for the business? (Yes or No)',\n",
        "    'Did the HDA accurately recap the conversation, provide a clear explanation of next steps setting expectations, and check for additional needs? (Yes or No)',\n",
        "    'Did the HDA thank the customer by name for their loyalty to the bank and close with appreciation for their business? (Yes or No)'\n",
        "    ]\n",
        "\n",
        "conversations = []\n",
        "for filename in conversation_files:\n",
        "    with open(os.path.join(conversation_folder, filename), \"r\") as file:\n",
        "        conversation = file.read()\n",
        "        conversations.append(conversation)\n",
        "\n",
        "data = pd.DataFrame({\"Conversation\": conversations})\n",
        "\n",
        "print(f\"loaded {len(data['Conversation'])} conversations\")\n",
        "print(f\"loaded {len(QUESTIONS)} questions\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/work_asu/config.json\", \"r\") as config_file:\n",
        "    config = json.load(config_file)\n",
        "openai.api_key = config[\"openai_api_key\"]\n",
        "print('openAi key loaded')\n",
        "\n",
        "cache = diskcache.Cache(\"/content/drive/MyDrive/work_asu\")\n",
        "print(\"cache loaded\")\n",
        "\n",
        "def promtAskAllQuestionForOneConversation(conversation):\n",
        "    prompt = '''\n",
        "    below is a customer service call between a Help Desk Agent(HDA) and Customer(C)\n",
        "\n",
        "    {}\n",
        "    based on the conversation above, answer these questions:\n",
        "\n",
        "    {}\n",
        "    Only give me the answers in order and put them in a list\n",
        "\n",
        "    '''.format(conversation, '\\n'.join(QUESTIONS))\n",
        "    logging.debug(\"engineered prompt: {}\".format(prompt))\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def get_openai_answer(conversation, modelName):\n",
        "    response = openai.Completion.create(\n",
        "      # model=\"text-davinci-003\",\n",
        "      model = modelName,\n",
        "      prompt=promtAskAllQuestionForOneConversation(conversation),\n",
        "      temperature=0,\n",
        "      max_tokens=50,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "    )\n",
        "    logging.info(response)\n",
        "    return response.choices[0].text\n",
        "\n",
        "def get_openai_answer_with_disk_cache(conversation, modelName):\n",
        "    cache_key = (conversation, modelName)\n",
        "    answers = ''\n",
        "    # Check if the answer is in the cache\n",
        "    if cache_key in cache:\n",
        "        print('in cache')\n",
        "        answers = cache[cache_key]\n",
        "    else:\n",
        "      print('not in cache, need to call openai')\n",
        "      # If not in cache, fetch the answer and store it in the cache\n",
        "      answers = get_openai_answer(conversation, modelName)  # Implement your OpenAI API call here\n",
        "      answers = answers.strip('\\n').strip('.').split(',')\n",
        "      if (len(answers)!= len(QUESTIONS)):\n",
        "            raise Exception(\"anwsers count invalid\")\n",
        "    cache[cache_key] = answers\n",
        "    print(answers)\n",
        "    return answers"
      ],
      "metadata": {
        "id": "cX0dViTVXx1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eebf74f-f905-4ef1-b049-4c6ee92101b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "loaded 10 conversations\n",
            "loaded 12 questions\n",
            "openAi key loaded\n",
            "cache loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### answer questions"
      ],
      "metadata": {
        "id": "cBobx6H3yUQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache.clear()\n",
        "data[\"OpenAI_Answers\"] = data[\"Conversation\"].apply(lambda conv: get_openai_answer_with_disk_cache(conv, 'text-davinci-003'))\n",
        "print(data)"
      ],
      "metadata": {
        "id": "u0ZvroZ-aI_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62ab7dd-83e9-4c2f-b175-c101a95bef05"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not in cache, need to call openai\n",
            "['Yes', ' Yes', ' Yes', ' No', ' NA', ' Yes', ' Yes', ' No', ' No', ' No', ' Yes', ' No']\n",
            "not in cache, need to call openai\n",
            "['Yes', ' Yes', ' Yes', ' No', ' NA', ' Yes', ' Yes', ' No', ' No', ' No', ' Yes', ' No']\n",
            "not in cache, need to call openai\n",
            "['Yes', ' Yes', ' Yes', ' Yes', ' NA', ' Yes', ' Yes', ' No', ' No', ' No', ' Yes', ' No']\n",
            "not in cache, need to call openai\n",
            "['Yes', ' Yes', ' Yes', ' No', ' NA', ' Yes', ' Yes', ' NA', ' No', ' No', ' Yes', ' No']\n",
            "not in cache, need to call openai\n",
            "['Yes', ' Yes', ' Yes', ' Yes', ' NA', ' Yes', ' Yes', ' NA', ' Yes', ' Yes', ' Yes', ' Yes']\n",
            "not in cache, need to call openai\n",
            "['Yes', ' Yes', ' Yes', ' No', ' NA', ' Yes', ' No', ' No', ' No', ' No', ' Yes', ' No']\n",
            "not in cache, need to call openai\n",
            "['Yes', ' Yes', ' Yes', ' No', ' NA', ' Yes', ' Yes', ' No', ' No', ' No', ' Yes', ' No']\n",
            "not in cache, need to call openai\n",
            "['Yes', ' Yes', ' Yes', ' No', ' NA', ' Yes', ' Yes', ' No', ' No', ' No', ' Yes', ' No']\n",
            "not in cache, need to call openai\n",
            "['Yes', ' Yes', ' Yes', ' Yes', ' NA', ' Yes', ' Yes', ' NA', ' Yes', ' Yes', ' Yes', ' No']\n",
            "not in cache, need to call openai\n",
            "['Yes', ' Yes', ' Yes', ' No', ' NA', ' Yes', ' No', ' NA', ' No', ' No', ' Yes', ' No']\n",
            "                                        Conversation  \\\n",
            "0  Help Desk Agent (HDA): Hello, thank you for ca...   \n",
            "1  Help Desk Agent (HDA): Thank you for calling t...   \n",
            "2  Help Desk Agent (HDA): Thank you for calling t...   \n",
            "3  Help Desk Agent (HDA): Thank you for calling t...   \n",
            "4  Help Desk Agent (HDA): Thank you for calling t...   \n",
            "5  Help Desk Agent (HDA): Thank you for calling t...   \n",
            "6  Help Desk Agent (HDA): Thank you for calling t...   \n",
            "7  Help Desk Agent (HDA): Thank you for calling t...   \n",
            "8  Help Desk Agent (HDA): Thank you for contactin...   \n",
            "9  Help Desk Agent (HDA): Thank you for calling t...   \n",
            "\n",
            "                                      OpenAI_Answers  \n",
            "0  [Yes,  Yes,  Yes,  No,  NA,  Yes,  Yes,  No,  ...  \n",
            "1  [Yes,  Yes,  Yes,  No,  NA,  Yes,  Yes,  No,  ...  \n",
            "2  [Yes,  Yes,  Yes,  Yes,  NA,  Yes,  Yes,  No, ...  \n",
            "3  [Yes,  Yes,  Yes,  No,  NA,  Yes,  Yes,  NA,  ...  \n",
            "4  [Yes,  Yes,  Yes,  Yes,  NA,  Yes,  Yes,  NA, ...  \n",
            "5  [Yes,  Yes,  Yes,  No,  NA,  Yes,  No,  No,  N...  \n",
            "6  [Yes,  Yes,  Yes,  No,  NA,  Yes,  Yes,  No,  ...  \n",
            "7  [Yes,  Yes,  Yes,  No,  NA,  Yes,  Yes,  No,  ...  \n",
            "8  [Yes,  Yes,  Yes,  Yes,  NA,  Yes,  Yes,  NA, ...  \n",
            "9  [Yes,  Yes,  Yes,  No,  NA,  Yes,  No,  NA,  N...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QVTPWXuO_nwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "NYG1oS_dikD2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-hNGul3kisHU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}